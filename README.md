# Information Retrieval Project - Enhanced with BM25

This project implements and compares three information retrieval models on the 20 Newsgroups dataset:
- **Vector Space Model (VSM)** with TF-IDF
- **BM25** (Best Matching 25) - Probabilistic ranking
- **Boolean Retrieval** with soft matching

## âš¡ Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/omaribrahim44/Information-Retrieval-System-for-20Newsgroups-Dataset.git
cd Information-Retrieval-System-for-20Newsgroups-Dataset

# 2. Install dependencies
pip install -r requirements.txt

# 3. Download NLTK data
python -c "import nltk; nltk.download('wordnet'); nltk.download('omw-1.4')"

# 4. Launch Jupyter Notebook
jupyter notebook

# 5. Open "IR PROJECT.ipynb" and run all cells (Cell â†’ Run All)
```

## ðŸ“‹ Features

### Retrieval Models
- âœ… Vector Space Model with TF-IDF and cosine similarity
- âœ… BM25 implementation with configurable parameters (k1, b)
- âœ… Boolean retrieval with AND/OR/NOT operators and soft matching fallback

### Evaluation & Analysis
- âœ… Comprehensive evaluation framework
  - Precision@k (k=5, 10, 20)
  - Recall@k (k=5, 10, 20)
  - Average Precision (AP)
  - Mean Average Precision (MAP)
- âœ… Per-category performance analysis
- âœ… Failure analysis (queries with AP < 0.2)
- âœ… Query length impact analysis

### Visualizations
- âœ… Precision@k comparison plots
- âœ… Recall@k comparison plots
- âœ… MAP comparison bar charts
- âœ… Per-category performance visualizations

### Export & Reporting
- âœ… Detailed CSV exports of all metrics
- âœ… Aggregated results summaries
- âœ… Automated markdown report generation
- âœ… High-resolution plots (300 DPI)

## ðŸš€ Getting Started

### Prerequisites

- **Python**: Version 3.7 or higher
- **pip**: Python package installer
- **Jupyter Notebook**: For running the notebook

### Installation

#### Step 1: Clone the Repository
```bash
git clone https://github.com/omaribrahim44/Information-Retrieval-System-for-20Newsgroups-Dataset.git
cd Information-Retrieval-System-for-20Newsgroups-Dataset
```

#### Step 2: Install Required Packages
```bash
pip install -r requirements.txt
```

This will install:
- `numpy` - Numerical computing
- `pandas` - Data manipulation and analysis
- `scikit-learn` - Machine learning library (includes TF-IDF, cosine similarity)
- `nltk` - Natural Language Toolkit (text preprocessing)
- `matplotlib` - Plotting and visualization
- `wordcloud` - Word cloud generation
- `jupyter` - Jupyter Notebook environment

#### Step 3: Download NLTK Data
The notebook requires NLTK's WordNet lemmatizer. Download it by running:

```bash
python -c "import nltk; nltk.download('wordnet'); nltk.download('omw-1.4')"
```

Or run this in a Python shell:
```python
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
```

### Running the Notebook

1. **Start Jupyter Notebook**:
   ```bash
   jupyter notebook
   ```

2. **Open the notebook**:
   - Navigate to `IR PROJECT.ipynb` in the Jupyter interface
   - Click to open

3. **Run all cells**:
   - Option 1: Click `Cell` â†’ `Run All` in the menu
   - Option 2: Run cells sequentially using `Shift + Enter`
   
   **Important**: Run cells in order from top to bottom!
   - Cells 1-43: Setup, preprocessing, and basic retrieval
   - Cells 44-70: BM25 implementation and enhanced analysis

4. **View results**:
   - Check visualizations in `ir_outputs/plots/`
   - Review CSV files in `ir_outputs/`
   - Read the summary report: `ir_outputs/evaluation_report.md`

### Expected Runtime

- **Full notebook execution**: ~5-10 minutes
- **BM25 evaluation** (Cell 52): ~2-3 minutes
- **Visualization generation**: ~10-15 seconds

## ðŸ“Š Dataset

**20 Newsgroups Dataset** - 5 selected categories:
- `alt.atheism`
- `comp.graphics`
- `rec.sport.baseball`
- `sci.med`
- `talk.politics.misc`

Total documents: 4,531

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ IR PROJECT.ipynb              # Main Jupyter notebook
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ Infor Retrieval Final project report.pdf
â”œâ”€â”€ Information retrieval project.pdf
â””â”€â”€ ir_outputs/                  # Generated results (created when run)
    â”œâ”€â”€ evaluation_detailed.csv
    â”œâ”€â”€ evaluation_summary.csv
    â”œâ”€â”€ category_performance.csv
    â”œâ”€â”€ map_scores.csv
    â”œâ”€â”€ evaluation_report.md
    â””â”€â”€ plots/
        â”œâ”€â”€ precision_comparison.png
        â”œâ”€â”€ recall_comparison.png
        â””â”€â”€ map_comparison.png
```

## ðŸ” Key Components

### BM25 Implementation
```python
def bm25_score(query_terms, doc_id, k1=1.5, b=0.75):
    # Computes BM25 score with:
    # - Term frequency saturation (k1)
    # - Document length normalization (b)
    # - IDF with smoothing
```

### Evaluation Framework
```python
def evaluate_all_models(queries, relevant_docs, k_values=[5, 10, 20]):
    # Evaluates VSM, BM25, and Boolean retrieval
    # Returns DataFrame with all metrics
```

## ðŸ“ˆ Results

The notebook generates comprehensive results including:
- **Performance Metrics**: Precision, Recall, AP, MAP for all models
- **Visualizations**: Comparison plots showing model performance
- **Analysis**: Per-category breakdown, failure analysis, query length impact
- **Reports**: Automated markdown summary with key findings

## ðŸ“š Documentation

All documentation is included in the Jupyter notebook with detailed markdown cells explaining each section.



## ðŸ“ Important Notes

- âš ï¸ **Run cells sequentially**: The notebook must be run from top to bottom
- âš ï¸ **Prerequisites required**: Cells 1-43 must be executed before cells 44-70
- â±ï¸ **Evaluation time**: The comprehensive evaluation (Cell 52) takes ~2-3 minutes
- ðŸ’¾ **Output location**: All results are saved to `ir_outputs/` directory
- ðŸ“Š **Dataset download**: The 20 Newsgroups dataset (~14 MB) downloads automatically on first run

## ðŸ› Troubleshooting

### Common Issues

**Issue**: `ModuleNotFoundError: No module named 'sklearn'`
```bash
# Solution: Install scikit-learn
pip install scikit-learn
```

**Issue**: `LookupError: Resource wordnet not found`
```python
# Solution: Download NLTK data
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
```

**Issue**: `NameError: name 'evaluation_queries' is not defined`
```
Solution: Run all cells from the beginning (cells 1-43 first)
```

**Issue**: Plots not displaying
```bash
# Solution: Ensure matplotlib is installed
pip install matplotlib
# If using Jupyter, add this to a cell:
%matplotlib inline
```

### Performance Tips

- Close other applications to free up memory
- The dataset requires ~500 MB RAM
- Evaluation of 50 queries processes ~4,500 documents each
- First run takes longer due to dataset download

## ðŸŽ“ Academic Context

This project was developed as part of an Information Retrieval course, demonstrating:
- Classical IR models (VSM, Boolean)
- Modern probabilistic ranking (BM25)
- Comprehensive evaluation methodologies
- Performance analysis and visualization

## ðŸ“„ License

This project is for educational purposes.

## ðŸ¤ Contributing

This is an academic project. For questions or suggestions, please open an issue.

---

**Last Updated**: December 2, 2024
